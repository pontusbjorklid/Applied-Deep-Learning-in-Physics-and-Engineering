{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB-sFvYtMKRK"
      },
      "source": [
        "# Exercise 7.5a Image segmentation with a U-Net architecture\n",
        "\n",
        "In this exercise you train an image segmentation model from scratch on the Oxford Pets dataset.\n",
        "https://www.robots.ox.ac.uk/~vgg/data/pets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utXSMN-4MKRO"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epOBy0TiMKRO"
      },
      "outputs": [],
      "source": [
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSgrXcuFMKRQ"
      },
      "source": [
        "## Prepare paths of input images and target segmentation masks\n",
        "\n",
        "you don't need to touch the code below. It creates lists of the filenames to the images and segmentation maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYvtzAwbMKRQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (160, 160) # all images get downscaled to this resolution\n",
        "num_classes = 3\n",
        "batch_size = 32\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmnEWSJMKRR"
      },
      "source": [
        "## What does one input image and corresponding segmentation mask look like?\n",
        "The codeblock below shows how you can display the images and the target segmentation mask. To reduce the computing load, we will downscale all images to a size of 160x160 pixels as defined above.\n",
        "The goal of this task is to predict the segmentation mask as precisely as possible.\n",
        "\n",
        "This example uses a few libraries to display and modify images (e.g. to rescale them to 160x160 pixels). The code below shows how these libraries can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSho9o8PMKRR"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "import numpy as np\n",
        "\n",
        "# Display input image #2 and #7\n",
        "for i_sample in [2,7, 3777]:\n",
        "  print(f\"image number {i_sample}\")\n",
        "  display(Image(filename=input_img_paths[i_sample]))\n",
        "\n",
        "  # Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "  # all pixels have either the value 1, 2 or 3:\n",
        "  # 1: Foreground 2:Background 3: Not classified\n",
        "  img =load_img(target_img_paths[i_sample])\n",
        "  display(PIL.ImageOps.autocontrast(img)) # to properly display the image, we set an autocontrast (otherwise the values 1,2,3 would just all be black)\n",
        "\n",
        "  print(f\"image number {i_sample} downscaled.\")\n",
        "  # the task is done on a downscaled version of the image\n",
        "  # the downscaling can be achieved by just passing a `targer_size` argument to the `load_img` function\n",
        "  display(load_img(input_img_paths[i_sample], target_size=img_size))\n",
        "  img = load_img(target_img_paths[i_sample], target_size=img_size)\n",
        "  display(PIL.ImageOps.autocontrast(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8pdRVWUMKRS"
      },
      "source": [
        "## Prepare `Sequence` class to load & vectorize batches of data\n",
        "You do not need to touch the codeblock below. It is a helper class that returns batches of images and their target masks in the downscaled version. This is an alternative way to provide the training and validation data to the KERAS fit function. A large library of images are typically too big too keep them all in memory. Instead, a so-called \"generator\" function returns a new batch of images everytime it is called. This is implemented below. When the `__get_item(idx)` method is called, it loads all images from batch `idx` into memory and returns it as a NumPy array.\n",
        "The `__len__` method returns how many batches are in one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wNR7yv2MKRS"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            y[j] = np.expand_dims(img, 2) # one hot encoding:\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            # i.e. background is 0, foreground (the animal) is 1, and unclassified is 3 (the contour around the animal)\n",
        "            y[j] -= 1\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT2SDZLYMKRT"
      },
      "source": [
        "## Prepare U-Net model\n",
        "\n",
        "Hints:\n",
        "* The final layer should have three feature maps with a softmax activation. This is because we want to predict the segmentation mask wich has three possible values: 0, 1, 2. The softmax activation works on each pixel, i.e., per pixel, the values of the feature maps add up to 1. Per pixel, the feature map with the highest propability indicates if we have \"background\", \"foreground\" or \"unclassified\". By using the `numpy.argmax` function, we can get back the integer for plotting the mask later (see `display_mask` function defined curther below).\n",
        "* instead of using standard convolutions you can use `SeparableConv2D` to reduce the number of trainable parameters\n",
        "* layers `x` and `x2` can be concacenated via `x = layers.concatenate([x, x2])`\n",
        "* upsampling can be done with `x = layers.UpSampling2D(2)(x)`\n",
        "* Always use `padding=\"same\"` to kee the spatial dimension constant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHksFdZeMKRU"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "\n",
        "# TODO: define a network with the UNet architecture below.\n",
        "inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "# Entry block: start by adding a convolution layer.\n",
        "x = layers.Conv2D(32, 3, padding=\"same\")(inputs)\n",
        "x = layers.BatchNormalization()(x) # using batch normalization after the convolution but before the activation function can help.\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "#TODO: Implement a UNet architecture here\n",
        "\n",
        "# Downsample through the model\n",
        "downsampled_layers = []\n",
        "for filters in [64, 128, 256]:\n",
        "    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    downsampled_layers.append(x)\n",
        "    x = layers.MaxPooling2D(2, strides=2, padding=\"same\")(x)\n",
        "\n",
        "x = layers.Conv2D(512, 3, padding=\"same\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "# Upsample and establish the skip connections\n",
        "for filters, skip_layer in zip([256, 128, 64], reversed(downsampled_layers)):\n",
        "    x = layers.Conv2DTranspose(filters, 2, strides=2, padding=\"same\")(x)\n",
        "    x = layers.Concatenate()([x, skip_layer])\n",
        "    x = layers.Conv2D(filters, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "# Add a per-pixel classification layer\n",
        "outputs = layers.Conv2D(37, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "# Define the model\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = layers.SeparableConv2D(64, # number of 1x1 convolutions / output channels,\n",
        "                    kernel_size=(3, 3), # kernel size for spatial convolution\n",
        "                    strides=(1, 1), # strides for spatial convolution\n",
        "                    padding='valid', # padding for spatial convolution\n",
        "                    depth_multiplier=1, # number of spatial convolutions per input channel\n",
        "                    )(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bt8vD1yMKRU"
      },
      "source": [
        "## Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jan2yqzMMKRV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = OxfordPets(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQjT9a-TMKRV"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYjFcwKgMKRV"
      },
      "outputs": [],
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "model.compile(..., loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 15\n",
        "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YTRPPeMMKRW"
      },
      "source": [
        "## Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyQeCS-cMKRW"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "model.load_weights(\"oxford_segmentation.h5\") # the last iteration might not be the best. So load back the best version\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "val_preds = model.predict(val_gen)\n",
        "\n",
        "\n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds[i], axis=-1) # find which feature map has the highest value per pixel -> this gives back the category 0, 1, or 2\n",
        "    mask = np.expand_dims(mask, axis=-1) # The image plotting library requires that the color of the image is given as an array (in our case it is just one number, but for color images we would have three color channels)\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNcq3oDy4wsD"
      },
      "outputs": [],
      "source": [
        "# Display results for validation image #10\n",
        "for i in range(10):\n",
        "\n",
        "  # Display input image\n",
        "  display(load_img(val_input_img_paths[i], target_size=img_size))\n",
        "\n",
        "  # Display ground-truth target mask\n",
        "  img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i], target_size=img_size))\n",
        "  display(img)\n",
        "\n",
        "  # Display mask predicted by our model\n",
        "  display_mask(i)  # Note that the model only sees inputs at 160x160."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
